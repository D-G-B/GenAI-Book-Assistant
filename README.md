# RAG Document Assistant

A production-ready Retrieval-Augmented Generation (RAG) system for intelligent document Q&A, built with FastAPI and LangChain.

## ğŸŒŸ Features

- **ğŸ“ Multi-Format Support**: Upload and process PDF, Word, Markdown, CSV, JSON, and text files
- **ğŸ’¬ Dual Chat Modes**: 
  - Simple Q&A for standalone questions
  - Conversational mode with memory for context-aware discussions
- **ğŸ” Smart Search**: Semantic search using FAISS vector database with HuggingFace embeddings
- **ğŸ¤– Multiple LLMs**: Support for OpenAI, Google Gemini, and Anthropic Claude
- **ğŸ“š Document Filtering**: Scope questions to specific documents
- **ğŸ’¾ Persistent Storage**: Vector embeddings saved to disk for fast restart
- **ğŸ¨ Web Interface**: Clean, modern UI for document management and chat

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Virtual environment (recommended)

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd <project-directory>
   ```

2. **Create and activate virtual environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   
   Create a `.env` file in the root directory:
   ```env
   # At least one API key is required
   OPENAI_API_KEY=your_openai_api_key_here
   GOOGLE_API_KEY=your_google_api_key_here
   ANTHROPIC_API_KEY=your_anthropic_api_key_here

   # Model configurations (optional)
   DEFAULT_OPENAI_MODEL=gpt-3.5-turbo
   DEFAULT_GEMINI_MODEL=gemini-pro
   DEFAULT_CLAUDE_MODEL=claude-3-opus-20240229

   # App settings (optional)
   DEBUG=False
   LOG_LEVEL=INFO
   MAX_TOKENS=1000
   ```

5. **Run the application**
   ```bash
   uvicorn main:app --reload
   ```

6. **Open your browser**
   
   Navigate to `http://localhost:8000`

## ğŸ“– Usage

### Document Upload

1. Click on the upload section in the sidebar
2. Choose a file (PDF, DOCX, TXT, MD, CSV, or JSON)
3. Optionally add a custom title
4. Click "Upload & Process"
5. Wait for processing to complete

### Asking Questions

**Simple Q&A Mode:**
- Each question is treated independently
- Best for factual queries about documents

**Conversational Mode:**
- Maintains context between questions
- Ideal for follow-up questions and discussions
- System remembers previous exchanges in the session

### Document Filtering

Use the dropdown menu to limit searches to specific documents, improving accuracy and speed.

## ğŸ—ï¸ Architecture

```
project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/              # API route handlers
â”‚   â”‚   â”œâ”€â”€ chat_routes.py
â”‚   â”‚   â”œâ”€â”€ conversational_routes.py
â”‚   â”‚   â””â”€â”€ documents_routes.py
â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”‚   â”œâ”€â”€ enhanced_rag_service.py
â”‚   â”‚   â”œâ”€â”€ conversational_memory.py
â”‚   â”‚   â””â”€â”€ advanced_document_loaders.py
â”‚   â”œâ”€â”€ schemas/          # Pydantic models
â”‚   â”œâ”€â”€ database.py       # SQLAlchemy setup
â”‚   â””â”€â”€ config.py         # Configuration management
â”œâ”€â”€ static/               # Frontend assets
â”œâ”€â”€ templates/            # HTML templates
â”œâ”€â”€ tests/                # Test utilities
â””â”€â”€ main.py              # FastAPI application
```

## ğŸ”§ Configuration

### Supported File Types

- **Text**: `.txt`, `.md`, `.markdown`
- **Documents**: `.pdf`, `.docx`, `.doc`
- **Data**: `.csv`, `.json`

### Embedding Model

Uses `all-MiniLM-L6-v2` from HuggingFace for generating embeddings (runs on CPU).

### Vector Storage

FAISS index is automatically saved to `./faiss_index/` for persistence across restarts.

## ğŸ“Š API Endpoints

### Documents
- `POST /api/v1/documents/upload-file` - Upload a document file
- `GET /api/v1/documents/list` - List all documents
- `DELETE /api/v1/documents/{id}` - Delete a document

### Chat
- `POST /api/v1/chat/ask` - Ask a question (simple mode)
- `POST /api/v1/conversation/ask` - Ask with conversation memory
- `GET /api/v1/conversation/history/{session_id}` - Get conversation history
- `GET /api/v1/chat/status` - Get system status

## ğŸ› ï¸ Development

### Adding New Document Types

Extend the `MultiFormatDocumentLoader` class in `app/services/advanced_document_loaders.py`:

```python
def _load_custom(self, file_path: str, metadata: Dict[str, Any]) -> List[Document]:
    # Your custom loader logic
    pass
```

### Customizing the Prompt

Modify the prompt template in `enhanced_rag_service.py`:

```python
prompt_template = """Your custom prompt here
Context: {context}
Question: {question}
Answer:"""
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- Built with [FastAPI](https://fastapi.tiangolo.com/)
- Powered by [LangChain](https://langchain.com/)
- Vector search by [FAISS](https://faiss.ai/)
- UI inspired by modern chat interfaces

## ğŸ“§ Contact

For questions or suggestions, please open an issue on GitHub.

---

**Note**: This is a learning project focused on RAG implementation and LLM integration. For production use, consider adding authentication, rate limiting, and comprehensive error handling.